# 什么是可观察性

 所有的流量都经过代理，因此代理对整个集群的访问情况知道得一清二楚，它把这些数据上报到控制中心，那么管理员就能观察到整个集群的流量情况。 

这里的可观察性主要指三个方面，log，metrics，tracing。

## 1. 日志

当流量流入服务网格中的微服务时，Istio可以为每个请求生成完整的记录，包括源和目标的元数据等。使运维人员能够将服务行为的审查控制到单个微服务的级别。

## 2. 监控

Istio基于监控的4 个黄金信号(延迟、流量、错误、饱和度)来生成一系列的服务指标，同时还提供了一组默认的服务网格监控大盘。

> 话外音：Istio还为服务网格控制平面提供了更为详细的监控指标。

## 3. 分布式跟踪

Istio根据采样率为每个请求生成完整的分布式追踪轨迹，以便运维人员可以理解服务网格内微服务的依赖关系和调用流程。

可以看出，Istio的可观察性，致力于解决两方面的问题：

1、症状：什么病？

- 是Istio的问题？
- 哪个Istio组件的问题？
- [...]

2、原因：为什么得这种病？

- 怎样跟踪、分析、定位？
- 是异常，还是偶然事件？
- [...]

知晓了症状(什么)和原因(为什么)，治病应该就信手拈来了吧，如果还不知如何治病，那就去格物致知吧。

# 链路跟踪

## zipkin

部署

```
 kubectl apply -f extras/zipkin.yaml -n istio-system
```

cm istio

```
[root@node01 ~]# kubectl get cm istio -n istio-system -o yaml
apiVersion: v1
data:
  mesh: |-
    accessLogFile: /dev/stdout
    enableTracing: true
    defaultConfig:
      discoveryAddress: istiod.istio-system.svc:15012
      meshId: mesh1
      proxyMetadata: {}
      tracing:
        sampling: 100
        zipkin:
          address: zipkin.istio-system:9411
    enablePrometheusMerge: true
    enableTracing: true
    rootNamespace: istio-system
    trustDomain: cluster.local
  meshNetworks: 'networks: {}'
```



暴露服务：

kubectl port-forward --address 0.0.0.0 -n istio-system zipkin-6b8c6bdc56-m2b4f 9411:9411



![1631513144(1)](images\1631513144(1).jpg)

![1631513293(1)](images\1631513293(1).jpg)

## jaeger

部署：

```
kubectl apply -f jaeger.yaml -n istio-system
```

![1631514048(1)](images\1631514048(1).jpg)



# ALS

skywalking

部署

```
helm repo add skywalking https://apache.jfrog.io/artifactory/skywalking-helm 

cd skywalking
  
helm install  skywalking -n istio-system \
  --set oap.env.SW_ENVOY_METRIC_ALS_HTTP_ANALYSIS=k8s-mesh \
  --set fullnameOverride=skywalking \
  --set oap.envoy.als.enabled=true \
  --set ui.image.tag=8.7.0 \
  --set oap.image.tag=8.7.0-es6 \
  --set oap.storageType=elasticsearch \
  --set ui.image.repository=apache/skywalking-ui \
  --set oap.image.repository=apache/skywalking-oap-server \
  .
  

暴露服务
kubectl port-forward --address 0.0.0.0 svc/skywalking-ui 8080:80 --namespace istio-system

配置istio

  enableEnvoyAccessLogService: true
  extensionProviders:
  - skywalking:
      service: skywalking-oap.istio-system.svc.cluster.local
      port: 11800
    name: envoy.tracers.skywalking
  defaultConfig:
    envoyAccessLogService:
      address: skywalking-oap.istio-system:11800

  mesh: |-
    accessLogFile: /dev/stdout
    enableEnvoyAccessLogService: true
    defaultConfig:
      envoyAccessLogService:
        address: skywalking-oap.istio-system:11800
      discoveryAddress: istiod.istio-system.svc:15012
      proxyMetadata: {}
      tracing:
        zipkin:
          address: zipkin.istio-system:9411
    enablePrometheusMerge: true
    rootNamespace: istio-system
    trustDomain: cluster.local
  meshNetworks: 'networks: {}'
    

删除：
helm uninstall  skywalking -n istio-system
```



# metrics



部署prometheus

```
kubectl apply -f prometheus.yaml -n istio-system
```

配置文件：

```
apiVersion: v1
kind: ConfigMap
metadata:
  labels:
    component: "server"
    app: prometheus
    release: prometheus
    chart: prometheus-11.16.2
    heritage: Helm
  name: prometheus
  namespace: istio-system
data:
  alerting_rules.yml: |
    {}
  alerts: |
    {}
  prometheus.yml: |
    global:
      evaluation_interval: 1m
      scrape_interval: 15s
      scrape_timeout: 10s
    rule_files:
    - /etc/config/recording_rules.yml
    - /etc/config/alerting_rules.yml
    - /etc/config/rules
    - /etc/config/alerts
    scrape_configs:
    - job_name: prometheus
      static_configs:
      - targets:
        - localhost:9090
    - bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
      job_name: kubernetes-apiservers
      kubernetes_sd_configs:
      - role: endpoints
      relabel_configs:
      - action: keep
        regex: default;kubernetes;https
        source_labels:
        - __meta_kubernetes_namespace
        - __meta_kubernetes_service_name
        - __meta_kubernetes_endpoint_port_name
      scheme: https
      tls_config:
        ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
        insecure_skip_verify: true
    - bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
      job_name: kubernetes-nodes
      kubernetes_sd_configs:
      - role: node
      relabel_configs:
      - action: labelmap
        regex: __meta_kubernetes_node_label_(.+)
      - replacement: kubernetes.default.svc:443
        target_label: __address__
      - regex: (.+)
        replacement: /api/v1/nodes/$1/proxy/metrics
        source_labels:
        - __meta_kubernetes_node_name
        target_label: __metrics_path__
      scheme: https
      tls_config:
        ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
        insecure_skip_verify: true
    - bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
      job_name: kubernetes-nodes-cadvisor
      kubernetes_sd_configs:
      - role: node
      relabel_configs:
      - action: labelmap
        regex: __meta_kubernetes_node_label_(.+)
      - replacement: kubernetes.default.svc:443
        target_label: __address__
      - regex: (.+)
        replacement: /api/v1/nodes/$1/proxy/metrics/cadvisor
        source_labels:
        - __meta_kubernetes_node_name
        target_label: __metrics_path__
      scheme: https
      tls_config:
        ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
        insecure_skip_verify: true
    - job_name: kubernetes-service-endpoints
      kubernetes_sd_configs:
      - role: endpoints
      relabel_configs:
      - action: keep
        regex: true
        source_labels:
        - __meta_kubernetes_service_annotation_prometheus_io_scrape
      - action: replace
        regex: (https?)
        source_labels:
        - __meta_kubernetes_service_annotation_prometheus_io_scheme
        target_label: __scheme__
      - action: replace
        regex: (.+)
        source_labels:
        - __meta_kubernetes_service_annotation_prometheus_io_path
        target_label: __metrics_path__
      - action: replace
        regex: ([^:]+)(?::\d+)?;(\d+)
        replacement: $1:$2
        source_labels:
        - __address__
        - __meta_kubernetes_service_annotation_prometheus_io_port
        target_label: __address__
      - action: labelmap
        regex: __meta_kubernetes_service_label_(.+)
      - action: replace
        source_labels:
        - __meta_kubernetes_namespace
        target_label: kubernetes_namespace
      - action: replace
        source_labels:
        - __meta_kubernetes_service_name
        target_label: kubernetes_name
      - action: replace
        source_labels:
        - __meta_kubernetes_pod_node_name
        target_label: kubernetes_node
    - job_name: kubernetes-service-endpoints-slow
      kubernetes_sd_configs:
      - role: endpoints
      relabel_configs:
      - action: keep
        regex: true
        source_labels:
        - __meta_kubernetes_service_annotation_prometheus_io_scrape_slow
      - action: replace
        regex: (https?)
        source_labels:
        - __meta_kubernetes_service_annotation_prometheus_io_scheme
        target_label: __scheme__
      - action: replace
        regex: (.+)
        source_labels:
        - __meta_kubernetes_service_annotation_prometheus_io_path
        target_label: __metrics_path__
      - action: replace
        regex: ([^:]+)(?::\d+)?;(\d+)
        replacement: $1:$2
        source_labels:
        - __address__
        - __meta_kubernetes_service_annotation_prometheus_io_port
        target_label: __address__
      - action: labelmap
        regex: __meta_kubernetes_service_label_(.+)
      - action: replace
        source_labels:
        - __meta_kubernetes_namespace
        target_label: kubernetes_namespace
      - action: replace
        source_labels:
        - __meta_kubernetes_service_name
        target_label: kubernetes_name
      - action: replace
        source_labels:
        - __meta_kubernetes_pod_node_name
        target_label: kubernetes_node
      scrape_interval: 5m
      scrape_timeout: 30s
    - honor_labels: true
      job_name: prometheus-pushgateway
      kubernetes_sd_configs:
      - role: service
      relabel_configs:
      - action: keep
        regex: pushgateway
        source_labels:
        - __meta_kubernetes_service_annotation_prometheus_io_probe
    - job_name: kubernetes-services
      kubernetes_sd_configs:
      - role: service
      metrics_path: /probe
      params:
        module:
        - http_2xx
      relabel_configs:
      - action: keep
        regex: true
        source_labels:
        - __meta_kubernetes_service_annotation_prometheus_io_probe
      - source_labels:
        - __address__
        target_label: __param_target
      - replacement: blackbox
        target_label: __address__
      - source_labels:
        - __param_target
        target_label: instance
      - action: labelmap
        regex: __meta_kubernetes_service_label_(.+)
      - source_labels:
        - __meta_kubernetes_namespace
        target_label: kubernetes_namespace
      - source_labels:
        - __meta_kubernetes_service_name
        target_label: kubernetes_name
    - job_name: kubernetes-pods
      kubernetes_sd_configs:
      - role: pod
      relabel_configs:
      - action: keep
        regex: true
        source_labels:
        - __meta_kubernetes_pod_annotation_prometheus_io_scrape
      - action: replace
        regex: (.+)
        source_labels:
        - __meta_kubernetes_pod_annotation_prometheus_io_path
        target_label: __metrics_path__
      - action: replace
        regex: ([^:]+)(?::\d+)?;(\d+)
        replacement: $1:$2
        source_labels:
        - __address__
        - __meta_kubernetes_pod_annotation_prometheus_io_port
        target_label: __address__
      - action: labelmap
        regex: __meta_kubernetes_pod_label_(.+)
      - action: replace
        source_labels:
        - __meta_kubernetes_namespace
        target_label: kubernetes_namespace
      - action: replace
        source_labels:
        - __meta_kubernetes_pod_name
        target_label: kubernetes_pod_name
      - action: drop
        regex: Pending|Succeeded|Failed
        source_labels:
        - __meta_kubernetes_pod_phase
    - job_name: kubernetes-pods-slow
      kubernetes_sd_configs:
      - role: pod
      relabel_configs:
      - action: keep
        regex: true
        source_labels:
        - __meta_kubernetes_pod_annotation_prometheus_io_scrape_slow
      - action: replace
        regex: (.+)
        source_labels:
        - __meta_kubernetes_pod_annotation_prometheus_io_path
        target_label: __metrics_path__
      - action: replace
        regex: ([^:]+)(?::\d+)?;(\d+)
        replacement: $1:$2
        source_labels:
        - __address__
        - __meta_kubernetes_pod_annotation_prometheus_io_port
        target_label: __address__
      - action: labelmap
        regex: __meta_kubernetes_pod_label_(.+)
      - action: replace
        source_labels:
        - __meta_kubernetes_namespace
        target_label: kubernetes_namespace
      - action: replace
        source_labels:
        - __meta_kubernetes_pod_name
        target_label: kubernetes_pod_name
      - action: drop
        regex: Pending|Succeeded|Failed
        source_labels:
        - __meta_kubernetes_pod_phase
      scrape_interval: 5m
      scrape_timeout: 30s
  recording_rules.yml: |
    {}
  rules: |
    {}
```



暴露服务

kubectl port-forward --address 0.0.0.0 -n istio-system prometheus-59d6f547cd-ckrsh 9090:9090



![1631511490(1)](images\1631511490(1).jpg)



# grafana

部署

```
kubectl apply -f grafana.yaml -n istio-system
```

暴露服务

```
kubectl port-forward --address 0.0.0.0 -n istio-system grafana-7bdcf77687-sznx4 3000:3000
```

![1631577843(1)](images\1631577843(1).jpg)

# kiali

部署：

```
 kubectl apply -f kiali.yaml -n istio-system
```

暴露服务

```
kubectl port-forward --address 0.0.0.0 -n istio-system kiali-84db5dd45-64bnb 20001:20001
```

![1631577389(1)](images\1631577389(1).jpg)



![1631577439(1)](images\1631577439(1).jpg)



![1631577481(1)](images\1631577481(1).jpg)



![1631577511(1)](images\1631577511(1).jpg)

![1631577555(1)](images\1631577555(1).jpg)

![1631577580(1)](images\1631577580(1).jpg)



# efk

部署

```
cd efk
helm install elasticsearch -n efk .
 helm install helm install kibana -n efk .
```



fluentd-config.yaml

kubectl apply -f fluentd-config.yaml -n istio

```
apiVersion: v1
kind: ConfigMap
metadata:
  name: fluentd-config
data:
  fluent.conf: |
    <source>
      @type tail
      format none
      path /var/log/p.log
      pos_file /var/log/p.log.pos
      tag count.format1
    </source>

    <match **>
      @type elasticsearch
      host elasticsearch-master.efk
      port 9200
      include_tag_key true
      tag_key @log_name
      logstash_format true
      flush_interval 10s
    </match>
```



 kubectl create clusterrolebinding bookinfo-productpage-admin --clusterrole=cluster-admin --serviceaccount=istio:bookinfo-productpage 





bookinfo-deploy.yaml

kubectl apply -f bookinfo-deploy.yaml -n istio

```
apiVersion: v1
kind: Service
metadata:
  name: productpage
  labels:
    app: productpage
    service: productpage
spec:
  ports:
  - port: 9080
    name: http
    protocol: TCP
    targetPort: 9080
  - targetPort: 24231
    name: http-fluentd
    protocol: TCP
    port: 24231
  selector:
    app: productpage
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: bookinfo-productpage
  labels:
    account: productpage
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: productpage-v1
  labels:
    app: productpage
    version: v1
spec:
  replicas: 1
  selector:
    matchLabels:
      app: productpage
      version: v1
  template:
    metadata:
      labels:
        app: productpage
        version: v1
    spec:
      serviceAccountName: bookinfo-productpage
      containers:
      - name: productpage
        image: registry.cn-hangzhou.aliyuncs.com/hxpdocker/examples-bookinfo-productpage:1.16.2
        imagePullPolicy: IfNotPresent
        ports:
        - containerPort: 9080
        volumeMounts:
        - name: tmp
          mountPath: /tmp
        - mountPath: /var/log
          name: varlog
        securityContext:
          runAsUser: 0
      - env:
        - name: FLUENT_ELASTICSEARCH_HOST
          value: elasticsearch-master.efk
        - name: FLUENT_ELASTICSEARCH_PORT
          value: "9200"
        image: fluent/fluentd-kubernetes-daemonset:v1-debian-elasticsearch
        imagePullPolicy: IfNotPresent
        name: fluentd
        ports:
        - containerPort: 24231
          name: metrics
          protocol: TCP
        volumeMounts:
        - mountPath: /var/log
          name: varlog
        - name: config-volume
          mountPath: /fluentd/etc/
      volumes:
       - name: varlog
         emptyDir: {}
       - name: config-volume
         configMap:
           name: fluentd-config
       - name: tmp
         emptyDir: {}
---
```

sc-productpage.yaml

kubectl apply -f sc-productpage.yaml -n istio

```
apiVersion: networking.istio.io/v1beta1
kind: Sidecar
metadata:
  name: productpage
spec:
  workloadSelector:
    labels:
      app: productpage
  egress:
  - hosts:
    - "efk/*"
    port:
      number: 9200
      protocol: HTTP
      name: egresshttp
    captureMode: NONE
```



![1631756521(1)](images\1631756521(1).jpg)



# 多集群可观测性

## 单控制面板

### 单网络

![arch (3)](images\arch (3).svg)

```
两集群网络联通
集群1
128,129,130
集群2
131,132,133

两个网络联通
128。129.130
route add -net 172.21.1.0 netmask 255.255.255.0 gw 192.168.229.131
route add -net 172.21.2.0 netmask 255.255.255.0 gw 192.168.229.133
route add -net 172.21.0.0 netmask 255.255.255.0 gw 192.168.229.132
route add -net 10.69.0.0 netmask 255.255.0.0 gw 192.168.229.131

131,132，133
route add -net 172.20.0.0 netmask 255.255.255.0 gw 192.168.229.128
route add -net 172.20.1.0 netmask 255.255.255.0 gw 192.168.229.129
route add -net 172.20.2.0 netmask 255.255.255.0 gw 192.168.229.130
route add -net 10.68.0.0 netmask 255.255.0.0 gw 192.168.229.128

生成部署operator文件
 cat <<EOF > cluster1.yaml
apiVersion: install.istio.io/v1alpha1
kind: IstioOperator
spec:
  profile: demo
  values:
    global:
      meshID: mesh1
      multiCluster:
        clusterName: cluster1
      network: network1
  meshConfig:
    accessLogFile: /dev/stdout
    enableTracing: true
  components:
    egressGateways:
    - name: istio-egressgateway
      enabled: true
EOF

这里我设置的cluster1东西向网关的ip试192.168.229.100
如果用的是loadblance，可以用下面命令获取
#  export DISCOVERY_ADDRESS=$(kubectl  -n istio-system get svc istio-eastwestgateway  -o jsonpath='{.status.loadBalancer.ingress[0].ip}')
然后替换remotePilotAddress

cat <<EOF > cluster2.yaml
apiVersion: install.istio.io/v1alpha1
kind: IstioOperator
spec:
  profile: demo
  values:
    global:
      meshID: mesh1
      multiCluster:
        clusterName: cluster2
      network: network1
      remotePilotAddress: 192.168.229.100
  meshConfig:
    accessLogFile: /dev/stdout
    enableTracing: true
  components:
    egressGateways:
    - name: istio-egressgateway
      enabled: true
EOF  

传输部署文件到另一个集群
scp cluster2.yaml root@192.168.229.131:/root

安装cluster1
istioctl install -f cluster1.yaml

生成东西向网关
/root/istio-1.11.2/samples/multicluster/gen-eastwest-gateway.sh --mesh mesh1 --cluster cluster1 --network network1 |  istioctl  install -y  -f -

 
配置东西向网关ip 
 kubectl patch svc  -n istio-system istio-eastwestgateway -p '{"spec":{"externalIPs":["192.168.229.100"]}}'
  
 暴露istiod
  kubectl apply  -n istio-system -f /root/istio-1.11.2/samples/multicluster/expose-istiod.yaml

cluster2:
生成访问apiserver secret
 istioctl x create-remote-secret --name=cluster2  --server=https://192.168.229.131:6443 > remote-secret-cluster2.yaml
 
 传输secret到cluster1
 scp remote-secret-cluster2.yaml root@192.168.229.128:/root
 
 cluster1:
 应用secret
  kubectl apply -f remote-secret-cluster2.yaml
 
 cluster2:
 安装cluster2
 istioctl install  -f cluster2.yaml
 
 
 cluster1: 
  重启pod
 kubectl rollout restart deploy -n istio
 kubectl rollout restart deploy -n istio-system
 
 cluster2:
   重启pod
 kubectl rollout restart deploy -n istio
 kubectl rollout restart deploy -n istio-system
```

cluster1部署zipkin

```
 kubectl apply -f extras/zipkin.yaml -n istio-system
```

cluster1增加东西向网关端口

```
 kubectl edit svc -n istio-system istio-eastwestgateway
 
  - name: http-zipkin
    nodePort: 32197
    port: 15018
    protocol: TCP
    targetPort: 15018
```

cluster1:

暴露zipkin

visilazation/zipkin-gw-vs.yaml

 kubectl apply -f zipkin-gw-vs.yaml -n istio-system

```
apiVersion: networking.istio.io/v1alpha3
kind: Gateway
metadata:
  name: zipkin-gateway
spec:
  selector:
    istio: eastwestgateway
  servers:
    - port:
        name: http-zipkin
        number: 15018
        protocol: http        
      hosts:
        - "*"
---
apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
  name: zipkin-vs
spec:
  hosts:
  - "*"
  gateways:
  - zipkin-gateway
  http:
  - route:
    - destination:
        host: zipkin.istio-system.svc.cluster.local
        port:
          number: 9411
```





cluster1: cm istio

```
[root@node01 ~]# kubectl get cm istio -n istio-system -o yaml
apiVersion: v1
data:
  mesh: |-
    accessLogFile: /dev/stdout
    enableTracing: true
    defaultConfig:
      discoveryAddress: istiod.istio-system.svc:15012
      meshId: mesh1
      proxyMetadata: {}
      tracing:
        sampling: 100
        zipkin:
          address: 192.168.229.100:15018
    enablePrometheusMerge: true
    enableTracing: true
    rootNamespace: istio-system
    trustDomain: cluster.local
  meshNetworks: 'networks: {}'
```

cluster2不需要配置



暴露服务：

kubectl port-forward --address 0.0.0.0 -n istio-system zipkin-6b8c6bdc56-m2b4f 9411:9411



![1631687902(1)](images\1631687902(1).jpg)



清理：

```
cluster1:

kubectl delete vs istiod-vs -n istio-system
kubectl delete gw istiod-gateway -n istio-system
kubectl delete secret istio-remote-secret-cluster2 -n istio-system
kubectl delete gw zipkin-gateway -n istio-system
kubectl delete vs zipkin-vs -n istio-system
istioctl x uninstall -f cluster1.yaml

reboot

cluster2:

istioctl x uninstall -f cluster2.yaml

reboot

```



### 多网络

![arch (2)](images\arch (2).svg)

```
集群1
128,129,130
集群2
131,132,133

给istio-system namespace 打标签
cluster1:
kubectl  label namespace istio-system topology.istio.io/network=network1
cluster2:
kubectl  label namespace istio-system topology.istio.io/network=network2

cluster1:
生成istio operator部署文件
cat <<EOF > cluster1.yaml
apiVersion: install.istio.io/v1alpha1
kind: IstioOperator
spec:
  profile: demo
  values:
    global:
      meshID: mesh1
      multiCluster:
        clusterName: cluster1
      network: network1
  meshConfig:
    accessLogFile: /dev/stdout
    enableTracing: true
  components:
    egressGateways:
    - name: istio-egressgateway
      enabled: true
EOF

这里我设置的cluster1东西向网关的ip试192.168.229.100
如果用的是loadblance，可以用下面命令获取
#  export DISCOVERY_ADDRESS=$(kubectl  -n istio-system get svc istio-eastwestgateway  -o jsonpath='{.status.loadBalancer.ingress[0].ip}')
然后替换remotePilotAddress

cat <<EOF > cluster2.yaml
apiVersion: install.istio.io/v1alpha1
kind: IstioOperator
spec:
  profile: demo
  values:
    global:
      meshID: mesh1
      multiCluster:
        clusterName: cluster2
      network: network2
      remotePilotAddress: 192.168.229.100
  meshConfig:
    accessLogFile: /dev/stdout
    enableTracing: true
  components:
    egressGateways:
    - name: istio-egressgateway
      enabled: true
EOF  

传输部署文件到另一个集群
scp cluster2.yaml root@192.168.229.131:/root

安装istio
istioctl install  -f cluster1.yaml

安装东西向网关
/root/istio-1.11.2/samples/multicluster/gen-eastwest-gateway.sh  --mesh mesh1 --cluster cluster1 --network network1 |  istioctl install -y  -f -
    
配置东西向网关ip 
 kubectl patch svc  -n istio-system istio-eastwestgateway -p '{"spec":{"externalIPs":["192.168.229.100"]}}'

暴露istiod
kubectl apply  -n istio-system -f /root/istio-1.11.2/samples/multicluster/expose-istiod.yaml
暴露服务
kubectl  apply -n istio-system -f /root/istio-1.11.2/samples/multicluster/expose-services.yaml

cluster2:
生成istiod访问apiserver secret
istioctl x create-remote-secret --name=cluster2  --server=https://192.168.229.131:6443 > remote-secret-cluster2.yaml

传输secret到cluster1
 scp remote-secret-cluster2.yaml root@192.168.229.128:/root
 
cluster1
安装secret
kubectl apply -f remote-secret-cluster2.yaml -n istio-system
 

cluster2:
部署cluster2
istioctl install  -f cluster2.yaml

生成东西向网关
/root/istio-1.11.2/samples/multicluster/gen-eastwest-gateway.sh --mesh mesh1 --cluster cluster2 --network network2 | istioctl install -y -f -

配置东西向网关ip 
 kubectl patch svc  -n istio-system istio-eastwestgateway -p '{"spec":{"externalIPs":["192.168.229.101"]}}'
 
暴露服务
kubectl  apply -n istio-system -f /root/istio-1.11.2/samples/multicluster/expose-services.yaml

重启pod
 kubectl rollout restart deploy -n istio
 kubectl rollout restart deploy -n istio-system
 
 cluster1:
 重启pod
 kubectl rollout restart deploy -n istio
 kubectl rollout restart deploy -n istio-system
```



cluster1部署zipkin

```
 kubectl apply -f extras/zipkin.yaml -n istio-system
```

cluster1增加东西向网关端口

```
 kubectl edit svc -n istio-system istio-eastwestgateway
 
  - name: http-zipkin
    nodePort: 32197
    port: 15018
    protocol: TCP
    targetPort: 15018
```

cluster1:

暴露zipkin

visilazation/zipkin-gw-vs.yaml

 kubectl apply -f zipkin-gw-vs.yaml -n istio-system

```
apiVersion: networking.istio.io/v1alpha3
kind: Gateway
metadata:
  name: zipkin-gateway
spec:
  selector:
    istio: eastwestgateway
  servers:
    - port:
        name: http-zipkin
        number: 15018
        protocol: http        
      hosts:
        - "*"
---
apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
  name: zipkin-vs
spec:
  hosts:
  - "*"
  gateways:
  - zipkin-gateway
  http:
  - route:
    - destination:
        host: zipkin.istio-system.svc.cluster.local
        port:
          number: 9411
```





cluster1,cluster2 : cm istio

```
[root@node01 ~]# kubectl get cm istio -n istio-system -o yaml
apiVersion: v1
data:
  mesh: |-
    accessLogFile: /dev/stdout
    enableTracing: true
    defaultConfig:
      discoveryAddress: istiod.istio-system.svc:15012
      meshId: mesh1
      proxyMetadata: {}
      tracing:
        sampling: 100
        zipkin:
          address: 192.168.229.100:15018
    enablePrometheusMerge: true
    enableTracing: true
    rootNamespace: istio-system
    trustDomain: cluster.local
  meshNetworks: 'networks: {}'
```



暴露服务：

kubectl port-forward --address 0.0.0.0 -n istio-system zipkin-6b8c6bdc56-m2b4f 9411:9411

<img src="images\1631613569(1).jpg" alt="1631613569(1)"  />

清理：

```
cluster1:

kubectl  label namespace istio-system topology.istio.io/network-
kubectl delete vs istiod-vs -n istio-system
kubectl delete gw istiod-gateway -n istio-system
kubectl delete gw cross-network-gateway -n istio-system
kubectl delete secret istio-remote-secret-cluster2 -n istio-system
kubectl delete gw zipkin-gateway -n istio-system
kubectl delete vs zipkin-vs -n istio-system
istioctl x uninstall -f cluster1.yaml

reboot

cluster2:

kubectl  label namespace istio-system topology.istio.io/network-
kubectl delete gw cross-network-gateway -n istio-system
istioctl x uninstall -f cluster2.yaml

reboot
```







## 多控制面板

### 多网络

**istio多集群部署**

![arch (1)](images\arch (1).svg)

```
集群1
128,129,130
集群2
131,132,133

给istio-system namespace打标签
cluster1:
 kubectl  label namespace istio-system topology.istio.io/network=network1
 cluster2:
 kubectl  label namespace istio-system topology.istio.io/network=network2
 
 cluster1:
 生成istio operator部署文件
 cat <<EOF > cluster1.yaml
apiVersion: install.istio.io/v1alpha1
kind: IstioOperator
spec:
  profile: demo
  values:
    global:
      meshID: mesh1
      multiCluster:
        clusterName: cluster1
      network: network1
  meshConfig:
    accessLogFile: /dev/stdout
    enableTracing: true
  components:
    egressGateways:
    - name: istio-egressgateway
      enabled: true
EOF

生成istio operator部署文件
cat <<EOF > cluster2.yaml
apiVersion: install.istio.io/v1alpha1
kind: IstioOperator
spec:
  profile: demo
  values:
    global:
      meshID: mesh1
      multiCluster:
        clusterName: cluster2
      network: network2
  meshConfig:
    accessLogFile: /dev/stdout
    enableTracing: true
  components:
    egressGateways:
    - name: istio-egressgateway
      enabled: true
EOF

传输部署文件到cluster2
scp cluster2.yaml root@192.168.229.131:/root

生成监控apiserver secret
 istioctl x create-remote-secret --name=cluster1  --server=https://192.168.229.128:6443 > remote-secret-cluster1.yaml
 传输secret到cluster2
scp remote-secret-cluster1.yaml root@192.168.229.131:/root

cluster2
 生成监控apiserver secret
 istioctl x create-remote-secret --name=cluster2  --server=https://192.168.229.131:6443 > remote-secret-cluster2.yaml
 
 传输secret到cluster1
 scp remote-secret-cluster2.yaml root@192.168.229.128:/root

cluster1:
部署监控apiserver secret
kubectl apply -f remote-secret-cluster2.yaml

部署istio
istioctl install  -f cluster1.yaml

部署东西向网关
/root/istio-1.11.2/samples/multicluster/gen-eastwest-gateway.sh --mesh mesh1 --cluster cluster1 --network network1 | istioctl  install -y  -f -

配置东西向网关ip 
 kubectl patch svc  -n istio-system istio-eastwestgateway -p '{"spec":{"externalIPs":["192.168.229.100"]}}'
 
 暴露服务
 kubectl  apply -n istio-system -f /root/istio-1.11.2/samples/multicluster/expose-services.yaml
 

cluster2:
部署监控apiserver secret
kubectl apply -f remote-secret-cluster1.yaml

部署istio
 istioctl install -f cluster2.yaml

部署东西向网关
 /root/istio-1.11.2/samples/multicluster/gen-eastwest-gateway.sh --mesh mesh1 --cluster cluster2 --network network2 |  istioctl install -y -f -

配置东西向网关ip 
 kubectl patch svc  -n istio-system istio-eastwestgateway -p '{"spec":{"externalIPs":["192.168.229.101"]}}'
 
 暴露服务
 kubectl  apply -n istio-system -f /root/istio-1.11.2/samples/multicluster/expose-services.yaml
 
 cluster1
 重启pod
 kubectl rollout restart deploy -n istio
 kubectl rollout restart deploy -n istio-system
 
 cluster2:
 重启pod
 kubectl rollout restart deploy -n istio
 kubectl rollout restart deploy -n istio-system
 
  验证
  kubectl exec  -n istio   "$(kubectl get pod  -n istio -l app=ratings -o jsonpath='{.items[0].metadata.name}')"  -- curl productpage.istio:9080/productpage
```



部署zipkin

```
 kubectl apply -f extras/zipkin.yaml -n istio-system
```

cluster1 : cm istio

```
[root@node01 ~]# kubectl get cm istio -n istio-system -o yaml
apiVersion: v1
data:
  mesh: |-
    accessLogFile: /dev/stdout
    enableTracing: true
    defaultConfig:
      discoveryAddress: istiod.istio-system.svc:15012
      meshId: mesh1
      proxyMetadata: {}
      tracing:
        sampling: 100
        zipkin:
          address: zipkin.istio-system:9411
    enablePrometheusMerge: true
    enableTracing: true
    rootNamespace: istio-system
    trustDomain: cluster.local
  meshNetworks: 'networks: {}'
```



暴露服务：

kubectl port-forward --address 0.0.0.0 -n istio-system zipkin-6b8c6bdc56-m2b4f 9411:9411



增加东西向网关端口

```
 kubectl edit svc -n istio-system istio-eastwestgateway
 
  - name: http-zipkin
    nodePort: 32197
    port: 15018
    protocol: TCP
    targetPort: 15018
```

暴露zipkin到cluster2

visilazation/zipkin-gw-vs.yaml

 kubectl apply -f zipkin-gw-vs.yaml -n istio-system

```
apiVersion: networking.istio.io/v1alpha3
kind: Gateway
metadata:
  name: zipkin-gateway
spec:
  selector:
    istio: eastwestgateway
  servers:
    - port:
        name: http-zipkin
        number: 15018
        protocol: http        
      hosts:
        - "*"
---
apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
  name: zipkin-vs
spec:
  hosts:
  - "*"
  gateways:
  - zipkin-gateway
  http:
  - route:
    - destination:
        host: zipkin.istio-system.svc.cluster.local
        port:
          number: 9411
```



**可以看到链路是不全的：**

![1631603597(1)](images\1631603597(1).jpg)



cluster2 : cm istio

```
[root@node01 ~]# kubectl get cm istio -n istio-system -o yaml
apiVersion: v1
data:
  mesh: |-
    accessLogFile: /dev/stdout
    enableTracing: true
    defaultConfig:
      discoveryAddress: istiod.istio-system.svc:15012
      meshId: mesh1
      proxyMetadata: {}
      tracing:
        sampling: 100
        zipkin:
          address: 192.168.229.100:15018
    enablePrometheusMerge: true
    enableTracing: true
    rootNamespace: istio-system
    trustDomain: cluster.local
  meshNetworks: 'networks: {}'
```

**可以看到链路是全的**



![1631605162(1)](images\1631605162(1).jpg)



清理：

```
cluster1:

kubectl  label namespace istio-system topology.istio.io/network-
kubectl delete gw cross-network-gateway -n istio-system
kubectl delete secret istio-remote-secret-cluster2 -n istio-system
kubectl delete gw zipkin-gateway -n istio-system
kubectl delete vs zipkin-vs -n istio-system
istioctl x uninstall -f cluster1.yaml

reboot

cluster2:

kubectl  label namespace istio-system topology.istio.io/network-
kubectl delete gw cross-network-gateway -n istio-system
kubectl delete secret istio-remote-secret-cluster1 -n istio-system
istioctl x uninstall -f cluster2.yaml

reboot
```



### 单网络

![arch (3)](images\arch (3).svg)



```
集群1
128,129,130
集群2
131,132,133

两个网络联通
128。129.130
route add -net 172.21.1.0 netmask 255.255.255.0 gw 192.168.229.131
route add -net 172.21.2.0 netmask 255.255.255.0 gw 192.168.229.133
route add -net 172.21.0.0 netmask 255.255.255.0 gw 192.168.229.132
route add -net 10.69.0.0 netmask 255.255.0.0 gw 192.168.229.131

131,132，133
route add -net 172.20.0.0 netmask 255.255.255.0 gw 192.168.229.128
route add -net 172.20.1.0 netmask 255.255.255.0 gw 192.168.229.129
route add -net 172.20.2.0 netmask 255.255.255.0 gw 192.168.229.130
route add -net 10.68.0.0 netmask 255.255.0.0 gw 192.168.229.128


cluster1:
生成istio安装operator文件
cat <<EOF > cluster1.yaml
apiVersion: install.istio.io/v1alpha1
kind: IstioOperator
spec:
  profile: demo
  values:
    global:
      meshID: mesh1
      multiCluster:
        clusterName: cluster1
      network: network1
  meshConfig:
    accessLogFile: /dev/stdout
    enableTracing: true
  components:
    egressGateways:
    - name: istio-egressgateway
      enabled: true
EOF


生成istio安装operator文件
cat <<EOF > cluster2.yaml
apiVersion: install.istio.io/v1alpha1
kind: IstioOperator
spec:
  profile: demo
  values:
    global:
      meshID: mesh1
      multiCluster:
        clusterName: cluster2
      network: network1
  meshConfig:
    accessLogFile: /dev/stdout
    enableTracing: true
  components:
    egressGateways:
    - name: istio-egressgateway
      enabled: true
EOF

把部署文件传到cluster2
scp cluster2.yaml root@192.168.229.131:/root


cluster1:
生成访问apiserver secret
 istioctl x create-remote-secret --name=cluster1  --server=https://192.168.229.128:6443 > remote-secret-cluster1.yaml
 传输secret到cluster2
scp remote-secret-cluster1.yaml root@192.168.229.131:/root

cluster2
生成访问apiserver secret
 istioctl x create-remote-secret --name=cluster2  --server=https://192.168.229.131:6443 > remote-secret-cluster2.yaml
  传输secret到cluster2
 scp remote-secret-cluster2.yaml root@192.168.229.128:/root
 
 cluster1
 应用secret
 kubectl apply -f remote-secret-cluster2.yaml
 
 部署集群
 istioctl install  -f cluster1.yaml
  
  
  cluster2:
  应用secret
  kubectl apply -f remote-secret-cluster1.yaml
  部署集群
istioctl install  -f cluster2.yaml

 cluster1: 
  重启pod
 kubectl rollout restart deploy -n istio
 kubectl rollout restart deploy -n istio-system
 
 cluster2:
   重启pod
 kubectl rollout restart deploy -n istio
 kubectl rollout restart deploy -n istio-system
```

部署zipkin

```
 kubectl apply -f extras/zipkin.yaml -n istio-system
```

由于cluster2 dns 无法解析zipkin.istio-system，所以cluster1需要安装东西向网关

```
部署东西向网关
/root/istio-1.11.2/samples/multicluster/gen-eastwest-gateway.sh --mesh mesh1 --cluster cluster1 --network network1 | istioctl  install -y  -f -

配置东西向网关ip 
 kubectl patch svc  -n istio-system istio-eastwestgateway -p '{"spec":{"externalIPs":["192.168.229.100"]}}'
```

cluster1增加东西向网关端口

```
 kubectl edit svc -n istio-system istio-eastwestgateway
 
  - name: http-zipkin
    nodePort: 32197
    port: 15018
    protocol: TCP
    targetPort: 15018
```

cluster1:

暴露zipkin

visilazation/zipkin-gw-vs.yaml

 kubectl apply -f zipkin-gw-vs.yaml -n istio-system

```
apiVersion: networking.istio.io/v1alpha3
kind: Gateway
metadata:
  name: zipkin-gateway
spec:
  selector:
    istio: eastwestgateway
  servers:
    - port:
        name: http-zipkin
        number: 15018
        protocol: http        
      hosts:
        - "*"
---
apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
  name: zipkin-vs
spec:
  hosts:
  - "*"
  gateways:
  - zipkin-gateway
  http:
  - route:
    - destination:
        host: zipkin.istio-system.svc.cluster.local
        port:
          number: 9411
```





cluster1,cluster2: cm istio

```
[root@node01 ~]# kubectl get cm istio -n istio-system -o yaml
apiVersion: v1
data:
  mesh: |-
    accessLogFile: /dev/stdout
    enableTracing: true
    defaultConfig:
      discoveryAddress: istiod.istio-system.svc:15012
      meshId: mesh1
      proxyMetadata: {}
      tracing:
        sampling: 100
        zipkin:
          address: 192.168.229.100:15018
    enablePrometheusMerge: true
    enableTracing: true
    rootNamespace: istio-system
    trustDomain: cluster.local
  meshNetworks: 'networks: {}'
```

```
 cluster1: 
  重启pod
 kubectl rollout restart deploy -n istio
 
 cluster2:
   重启pod
 kubectl rollout restart deploy -n istio
```



暴露服务：

kubectl port-forward --address 0.0.0.0 -n istio-system zipkin-6b8c6bdc56-m2b4f 9411:9411



![1631689597(1)](images\1631689597(1).jpg)



清理：

```
cluster1:

kubectl delete secret istio-remote-secret-cluster2 -n istio-system
kubectl delete gw zipkin-gateway -n istio-system
kubectl delete vs zipkin-vs -n istio-system
istioctl x uninstall -f cluster1.yaml


reboot

cluster2:


kubectl delete secret istio-remote-secret-cluster1 -n istio-system
istioctl x uninstall -f cluster2.yaml

reboot
```

